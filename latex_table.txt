\begin{table}
\begin{adjustbox}{width=\textwidth, pagecenter}
\begin{tabular}{ccccc|c}
\toprule
dropout rate & optimizer & learning rate & batch size & nb epochs & accuracy \\ 
\midrule
0 & Adam & 0.001 & 2 & 6 & 48.80 \\ 
0 & Adam & 0.001 & 8 & 14 & 47.53 \\ 
0 & Adam & 0.001 & 32 & 17 & 55.07 \\ 
0 & Adam & 0.001 & 128 & 27 & 50.73 \\ 
0 & Adam & 0.0001 & 2 & 52 & 74.23 \\ 
0 & Adam & 0.0001 & 8 & 23 & 37.53 \\ 
0 & Adam & 0.0001 & 32 & 41 & 50.20 \\ 
0 & Adam & 0.0001 & 128 & 52 & 35.67 \\ 
0 & SGD & 0.001 & 2 & 12 & 10.63 \\ 
0 & SGD & 0.001 & 8 & 11 & 9.30 \\ 
0 & SGD & 0.001 & 32 & 1 & 5.20 \\ 
0 & SGD & 0.001 & 128 & 1 & 5.20 \\ 
0 & SGD & 0.0001 & 2 & 1 & 5.20 \\ 
0 & SGD & 0.0001 & 8 & 1 & 5.20 \\ 
0 & SGD & 0.0001 & 32 & 1 & 5.20 \\ 
0 & SGD & 0.0001 & 128 & 1 & 5.20 \\ 
0.2 & Adam & 0.001 & 2 & 6 & 31.80 \\ 
0.2 & Adam & 0.001 & 8 & 1 & 9.30 \\ 
0.2 & Adam & 0.001 & 32 & 32 & 66.73 \\ 
\textbf{0.2} & \textbf{Adam} & \textbf{0.001} & \textbf{128} & \textbf{51} & \textbf{75.80} \\ 
0.2 & Adam & 0.0001 & 2 & 58 & 68.43 \\ 
0.2 & Adam & 0.0001 & 8 & 52 & 60.83 \\ 
0.2 & Adam & 0.0001 & 32 & 60 & 54.67 \\ 
0.2 & Adam & 0.0001 & 128 & 77 & 58.30 \\ 
0.2 & SGD & 0.001 & 2 & 8 & 10.47 \\ 
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Parameter tuning results for nn on the MNIST dataset.}
\label{tab:parameter_tuning_nn_MNIST}
\end{table}
